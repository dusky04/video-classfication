{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f9e928fc-73bf-458b-86d3-90ed3511c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50bf2765-b020-42fd-ac43-13cd88649af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global File Paths\n",
    "ZIP_FILES_PATH = \"data\"\n",
    "DATASET_NAME = \"CricShot10\"\n",
    "DATASET_ROOT_PATH = \"CricShot10/\"\n",
    "TO_DIR = \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eff8b7c-4361-4cc0-97de-6d3a6e807c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SET_RATIO = 0.8\n",
    "TEST_SET_RATIO = 1 - TRAIN_SET_RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c6a47c18-baf8-4f48-a730-9739ebbb4a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FRAMES = 10\n",
    "FRAME_SHAPE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0fcd4d40-6d46-406e-9342-7c1e90f033e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "504b38bb-2fe7-4cda-9ee6-e219da9d066d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory: CricShot10\n",
      "Unzipping all the files: \n",
      "Unzipping: cover-20250328T152434Z-001.zip\n",
      "Unzipping: defense-20250328T152432Z-001.zip\n",
      "Unzipping: flick-20250328T152430Z-001.zip\n",
      "Unzipping: hook-20250328T152430Z-001.zip\n",
      "Unzipping: late_cut-20250328T152307Z-001.zip\n",
      "Unzipping: lofted-20250328T152550Z-001.zip\n",
      "Unzipping: pull-20250328T152305Z-001.zip\n",
      "Unzipping: square_cut-20250328T152247Z-001.zip\n",
      "Unzipping: straight-20250328T152224Z-001.zip\n",
      "Unzipping: sweep-20250328T152202Z-001.zip\n",
      "All files unzipped to: `C:\\Users\\Vaibhav Rastogi\\Documents\\projects\\CricShot10`\n"
     ]
    }
   ],
   "source": [
    "# Extracting all files\n",
    "utils.unzip_files(ZIP_FILES_PATH, DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "155529f0-78d1-4ce4-90e1-e1f4b366d323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Number of samples per class taken: 179\n"
     ]
    }
   ],
   "source": [
    "samples_per_class = min(\n",
    "    (\n",
    "        len(list(path.glob(\"*.avi\")))\n",
    "        for path in Path(DATASET_ROOT_PATH).iterdir()\n",
    "        if path.is_dir()\n",
    "    )\n",
    ")\n",
    "print(\"Minimum Number of samples per class taken:\", samples_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d173490-1bc6-44ed-898c-30e406487845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the directory structure with defined train and test directories\n",
    "utils.setup_dataset_structure(\n",
    "    from_dir=DATASET_ROOT_PATH,\n",
    "    to_dir=TO_DIR,\n",
    "    train_ratio=TRAIN_SET_RATIO,\n",
    "    samples_per_class=samples_per_class,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cb73b0d1-aa9a-44a3-b33d-15a7a9927ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train and test directories\n",
    "root_dir = Path(TO_DIR)\n",
    "train_dir = root_dir / \"train\"\n",
    "test_dir = root_dir / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "088ffc08-af18-4caf-b06c-92a7e37283fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of video files in training set: 1430\n",
      "No. of video files in testing set: 360\n"
     ]
    }
   ],
   "source": [
    "train_paths = list(train_dir.glob(\"*/*.avi\"))\n",
    "test_paths = list(test_dir.glob(\"*/*.avi\"))\n",
    "print(\"No. of video files in training set:\", len(train_paths))\n",
    "print(\"No. of video files in testing set:\", len(test_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "33c23646-2055-433f-a99d-daef23fb1866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Classes in dataset: ['cover', 'defense', 'flick', 'hook', 'late_cut', 'lofted', 'pull', 'square_cut', 'straight', 'sweep']\n"
     ]
    }
   ],
   "source": [
    "# Get all the classes in the dataset\n",
    "class_names, classes_to_idx = utils.get_classes(train_dir)\n",
    "idx_to_classes = {idx: class_name for class_name, idx in classes_to_idx.items()}\n",
    "print(\"All Classes in dataset:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "adf87e8d-3efe-4f4b-9d66-63471bfcd2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CricShot10(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_dir: str,\n",
    "        transform=None,\n",
    "        n_frames: int = 16,\n",
    "        target_size: Tuple[int, int] = (224, 224),\n",
    "    ):\n",
    "        self.target_dir = target_dir\n",
    "        self.paths = list(target_dir.glob(\"*/*.avi\"))\n",
    "\n",
    "        self.transform = transform\n",
    "        self.n_frames = n_frames\n",
    "        self.target_size = target_size\n",
    "        self.class_names, self.class_to_idx = utils.get_classes(target_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def load_video(self, idx: int) -> torch.Tensor:\n",
    "        video_path = str(self.paths[idx])\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "        indices = np.linspace(0, frame_count - 1, self.n_frames, dtype=\"int\")\n",
    "\n",
    "        frames = []\n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                last_frame = (\n",
    "                    frames[-1]\n",
    "                    if frames\n",
    "                    else np.zeros(\n",
    "                        (self.target_size[0], self.target_size[1], 3), dtype=np.uint8\n",
    "                    )\n",
    "                )\n",
    "                frames.append(last_frame)\n",
    "                continue\n",
    "\n",
    "            frame = cv2.resize(frame, self.target_size)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) < self.n_frames:\n",
    "            # Pad with copies of the last frame\n",
    "            last_frame = (\n",
    "                frames[-1]\n",
    "                if frames\n",
    "                else np.zeros(\n",
    "                    (self.target_size[0], self.target_size[1], 3), dtype=np.uint8\n",
    "                )\n",
    "            )\n",
    "            frames.extend([last_frame] * (self.n_frames - len(frames)))\n",
    "\n",
    "        # Pytorch requires (C, D, H, W)\n",
    "        frames = np.transpose(np.array(frames), (3, 0, 2, 1))\n",
    "        return frames\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        video = self.load_video(idx)\n",
    "        class_name = \"_\".join(self.paths[idx].name.split(\"_\")[:-1])\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        if self.transform:\n",
    "            return self.transform(video), class_idx\n",
    "        return video, class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e7c859e5-20be-4064-84d5-ff79cb3c6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transform = transforms.Compose(\n",
    "#     [\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# test_transform = transforms.Compose(\n",
    "#     [\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c1806dd6-3c76-4904-88df-fc5b1c0069ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CricShot10(\n",
    "    train_dir,\n",
    "    n_frames=N_FRAMES,\n",
    "    target_size=FRAME_SHAPE,\n",
    "    # transform=test_transform\n",
    ")\n",
    "\n",
    "test_dataset = CricShot10(\n",
    "    test_dir,\n",
    "    n_frames=N_FRAMES,\n",
    "    target_size=FRAME_SHAPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "58f816bc-5f99-447c-82d1-ce22eed32405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloaders\n",
    "# Shape: [10, 3, 16, 224, 224]\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "132e37ce-b119-43dc-bbf6-020c3241bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2Plus1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        out_channels: int = 16,\n",
    "        kernel_size: Tuple[int, int, int] = (3, 7, 7),\n",
    "        # padding\n",
    "    ):\n",
    "        super.__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            # Spatial decomposition\n",
    "            nn.Conv3d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=(1, kernel_size[1], kernel_size[2]),\n",
    "                padding=\"same\",\n",
    "            ),\n",
    "            # Temporal decomposition\n",
    "            nn.Conv3d(\n",
    "                in_channels=out_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=(kernel_size[0], 1, 1),\n",
    "                padding=\"same\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ResidualMain(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        out_channels: int = 16,\n",
    "        kernel_size: Tuple[int, int, int] = (3, 7, 7),\n",
    "    ):\n",
    "        super.__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            Conv2Plus1D(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=\"same\",\n",
    "            ),\n",
    "            nn.LayerNorm(),\n",
    "            nn.ReLU(),\n",
    "            Conv2Plus1D(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=\"same\",\n",
    "            ),\n",
    "            nn.LayerNorm(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class Project(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super.__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(in_features=in_features, out_features=out_features),\n",
    "            nn.LayerNorm(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3a4a629d-2b3b-4af2-a988-90087fc64a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int = 3, kernel_size: Tuple[int, int, int] = (3, 7, 7)\n",
    "    ):\n",
    "        super.__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027bcd3d-2202-4e68-ac0c-c83e018a1a63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
