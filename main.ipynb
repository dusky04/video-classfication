{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673e3276-b4c1-47ed-b06f-a5a0f68bc3bc",
   "metadata": {},
   "source": [
    "### CricShot10 Video Classification\n",
    "\n",
    "(Use this with permission)\n",
    "- `Source` - https://drive.google.com/drive/folders/1DPHURwQk5R8blgjM8VNz6Q68LqckxljX?usp=drive_link\n",
    "\n",
    "\n",
    "Notebook to set the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e928fc-73bf-458b-86d3-90ed3511c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dependencies\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# Contains helper functions to parse the dataset\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50bf2765-b020-42fd-ac43-13cd88649af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global File Paths\n",
    "# Change them before running the notebook\n",
    "# These all are specific to my system\n",
    "ZIP_FILES_PATH = \"zipped-data\"\n",
    "DATASET_NAME = \"CricShot10\"\n",
    "DATASET_ROOT_PATH = Path(\"CricShot10/\")\n",
    "TO_DIR = \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eff8b7c-4361-4cc0-97de-6d3a6e807c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% of videos are used for training the model\n",
    "TRAIN_SET_RATIO = 0.8\n",
    "TEST_SET_RATIO = 1 - TRAIN_SET_RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6a47c18-baf8-4f48-a730-9739ebbb4a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority of videos are 25FPS with a few being 30FPS\n",
    "# Uniformly Sampling 10 frames from each videos\n",
    "N_FRAMES = 10\n",
    "# Change each frame (which is basically a picture) to (height, width) - (224, 224)\n",
    "FRAME_SHAPE = (224, 224)\n",
    "# Each batch contains 16 videos\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "504b38bb-2fe7-4cda-9ee6-e219da9d066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all files\n",
    "# Only needs to run once if the data is zipped\n",
    "if not DATASET_ROOT_PATH.exists() and not DATASET_ROOT_PATH.is_dir():\n",
    "    utils.unzip_files(ZIP_FILES_PATH, DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d173490-1bc6-44ed-898c-30e406487845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found dataset at: CricShot10\n",
      "LOG: Creating directory 'dataset'\n",
      "LOG: Creating directory 'dataset\\train'\n",
      "LOG: Creating directory 'dataset\\test'\n",
      "Warning: Only found 9 videos in CricShot10\\cover, expected 179.\n",
      "Warning: Only found 13 videos in CricShot10\\defense, expected 179.\n",
      "Warning: Only found 2 videos in CricShot10\\flick, expected 179.\n",
      "Warning: Only found 2 videos in CricShot10\\hook, expected 179.\n",
      "Warning: Only found 3 videos in CricShot10\\late_cut, expected 179.\n",
      "Warning: Only found 19 videos in CricShot10\\lofted, expected 179.\n",
      "Warning: Only found 0 videos in CricShot10\\pull, expected 179.\n",
      "Warning: Only found 21 videos in CricShot10\\square_cut, expected 179.\n",
      "Warning: Only found 14 videos in CricShot10\\straight, expected 179.\n",
      "Warning: Only found 15 videos in CricShot10\\sweep, expected 179.\n"
     ]
    }
   ],
   "source": [
    "# Setup the directory structure with defined train and test directories\n",
    "# Only needs to run once to setup the dataset directory\n",
    "utils.setup_dataset_structure(\n",
    "    from_dir=DATASET_ROOT_PATH,\n",
    "    to_dir=TO_DIR,\n",
    "    train_ratio=TRAIN_SET_RATIO,\n",
    "    samples_per_class=samples_per_class,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "155529f0-78d1-4ce4-90e1-e1f4b366d323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Number of samples per class taken: 0\n"
     ]
    }
   ],
   "source": [
    "# Our dataset does not uniform distribution of data in each of the 10 classes\n",
    "# Waisting the only the minimum amount of videos\n",
    "samples_per_class = min(\n",
    "    (\n",
    "        len(list(path.glob(\"*.avi\")))\n",
    "        for path in Path(DATASET_ROOT_PATH).iterdir()\n",
    "        if path.is_dir()\n",
    "    )\n",
    ")\n",
    "print(\"Minimum Number of samples per class taken:\", samples_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb73b0d1-aa9a-44a3-b33d-15a7a9927ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train and test directories\n",
    "root_dir = Path(TO_DIR)\n",
    "train_dir = root_dir / \"train\"\n",
    "test_dir = root_dir / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "088ffc08-af18-4caf-b06c-92a7e37283fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of video files in training set: 1528\n",
      "No. of video files in testing set: 360\n"
     ]
    }
   ],
   "source": [
    "train_paths = list(train_dir.glob(\"*/*.avi\"))\n",
    "test_paths = list(test_dir.glob(\"*/*.avi\"))\n",
    "print(\"No. of video files in training set:\", len(train_paths))\n",
    "print(\"No. of video files in testing set:\", len(test_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33c23646-2055-433f-a99d-daef23fb1866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Classes in dataset: ['cover', 'defense', 'flick', 'hook', 'late_cut', 'lofted', 'pull', 'square_cut', 'straight', 'sweep']\n"
     ]
    }
   ],
   "source": [
    "# Get all the classes in the dataset\n",
    "class_names, classes_to_idx = utils.get_classes(train_dir)\n",
    "idx_to_classes = {idx: class_name for class_name, idx in classes_to_idx.items()}\n",
    "print(\"All Classes in dataset:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adf87e8d-3efe-4f4b-9d66-63471bfcd2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CricShot10(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_dir: str,\n",
    "        transform=None,\n",
    "        n_frames: int = 16,\n",
    "        target_size: Tuple[int, int] = (224, 224),\n",
    "    ):\n",
    "        self.target_dir = target_dir\n",
    "        self.paths = list(target_dir.glob(\"*/*.avi\"))\n",
    "\n",
    "        self.transform = transform\n",
    "        self.n_frames = n_frames\n",
    "        self.target_size = target_size\n",
    "        self.class_names, self.class_to_idx = utils.get_classes(target_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def load_video(self, idx: int) -> torch.Tensor:\n",
    "        video_path = str(self.paths[idx])\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "        indices = np.linspace(0, frame_count - 1, self.n_frames, dtype=\"int\")\n",
    "\n",
    "        frames = []\n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                last_frame = (\n",
    "                    frames[-1]\n",
    "                    if frames\n",
    "                    else np.zeros(\n",
    "                        (self.target_size[0], self.target_size[1], 3), dtype=np.uint8\n",
    "                    )\n",
    "                )\n",
    "                frames.append(last_frame)\n",
    "                continue\n",
    "\n",
    "            frame = cv2.resize(frame, self.target_size)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) < self.n_frames:\n",
    "            # Pad with copies of the last frame\n",
    "            last_frame = (\n",
    "                frames[-1]\n",
    "                if frames\n",
    "                else np.zeros(\n",
    "                    (self.target_size[0], self.target_size[1], 3), dtype=np.uint8\n",
    "                )\n",
    "            )\n",
    "            frames.extend([last_frame] * (self.n_frames - len(frames)))\n",
    "\n",
    "        # Pytorch requires (C, D, H, W)\n",
    "        frames = np.transpose(np.array(frames), (3, 0, 2, 1))\n",
    "        return frames\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        video = self.load_video(idx)\n",
    "        class_name = \"_\".join(self.paths[idx].name.split(\"_\")[:-1])\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        if self.transform:\n",
    "            return self.transform(video), class_idx\n",
    "        return video, class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7c859e5-20be-4064-84d5-ff79cb3c6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transform = transforms.Compose(\n",
    "#     [\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# test_transform = transforms.Compose(\n",
    "#     [\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1806dd6-3c76-4904-88df-fc5b1c0069ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CricShot10(\n",
    "    train_dir,\n",
    "    n_frames=N_FRAMES,\n",
    "    target_size=FRAME_SHAPE,\n",
    "    # transform=test_transform\n",
    ")\n",
    "\n",
    "test_dataset = CricShot10(\n",
    "    test_dir,\n",
    "    n_frames=N_FRAMES,\n",
    "    target_size=FRAME_SHAPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58f816bc-5f99-447c-82d1-ce22eed32405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloaders\n",
    "# Shape: [10, 3, 16, 224, 224]\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "132e37ce-b119-43dc-bbf6-020c3241bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2Plus1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        out_channels: int = 16,\n",
    "        kernel_size: Tuple[int, int, int] = (3, 7, 7),\n",
    "        # padding\n",
    "    ):\n",
    "        super.__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            # Spatial decomposition\n",
    "            nn.Conv3d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=(1, kernel_size[1], kernel_size[2]),\n",
    "                padding=\"same\",\n",
    "            ),\n",
    "            # Temporal decomposition\n",
    "            nn.Conv3d(\n",
    "                in_channels=out_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=(kernel_size[0], 1, 1),\n",
    "                padding=\"same\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ResidualMain(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        out_channels: int = 16,\n",
    "        kernel_size: Tuple[int, int, int] = (3, 7, 7),\n",
    "    ):\n",
    "        super.__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            Conv2Plus1D(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=\"same\",\n",
    "            ),\n",
    "            nn.LayerNorm(),\n",
    "            nn.ReLU(),\n",
    "            Conv2Plus1D(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=\"same\",\n",
    "            ),\n",
    "            nn.LayerNorm(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class Project(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super.__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(in_features=in_features, out_features=out_features),\n",
    "            nn.LayerNorm(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a4a629d-2b3b-4af2-a988-90087fc64a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int = 3, kernel_size: Tuple[int, int, int] = (3, 7, 7)\n",
    "    ):\n",
    "        super.__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027bcd3d-2202-4e68-ac0c-c83e018a1a63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
